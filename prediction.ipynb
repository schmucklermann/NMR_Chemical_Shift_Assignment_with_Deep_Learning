{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "747a0aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from pathlib import Path\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "from random import randint\n",
    "import random\n",
    "import seaborn as sns\n",
    "import bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96b89d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "# https://discuss.pytorch.org/t/reproducibility-with-all-the-bells-and-whistles/81097\n",
    "def seed_all(seed=42):\n",
    "    print(\"[ Using Seed : \", seed, \" ]\")\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ebb81d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the linear regression model\n",
    "class LinearRegression(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(1024, 2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "\n",
    "# Define a two-layer FNN model\n",
    "class TwoLayerFNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(TwoLayerFNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create or load your 1D CNN model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=128, kernel_size=7, padding=3)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(in_channels=128, out_channels=32, kernel_size=7, padding=3)\n",
    "        self.fc = nn.Linear(32, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0,2,1) \n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.permute(0,2,1)\n",
    "        x = self.fc(x)  # Take the last output of the convolutional layers\n",
    "        return x\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d86d985e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.gridspec as gridspec\n",
    "class SeabornFig2Grid():\n",
    "    def __init__(self, seaborngrid, fig,  subplot_spec):\n",
    "        self.fig = fig\n",
    "        self.sg = seaborngrid\n",
    "        self.subplot = subplot_spec\n",
    "        if isinstance(self.sg, sns.axisgrid.FacetGrid) or \\\n",
    "            isinstance(self.sg, sns.axisgrid.PairGrid):\n",
    "            self._movegrid()\n",
    "        elif isinstance(self.sg, sns.axisgrid.JointGrid):\n",
    "            self._movejointgrid()\n",
    "        self._finalize()\n",
    "    def _movegrid(self):\n",
    "        \"\"\" Move PairGrid or Facetgrid \"\"\"\n",
    "        self._resize()\n",
    "        n = self.sg.axes.shape[0]\n",
    "        m = self.sg.axes.shape[1]\n",
    "        self.subgrid = gridspec.GridSpecFromSubplotSpec(n,m, subplot_spec=self.subplot)\n",
    "        for i in range(n):\n",
    "            for j in range(m):\n",
    "                self._moveaxes(self.sg.axes[i,j], self.subgrid[i,j])\n",
    "    def _movejointgrid(self):\n",
    "        \"\"\" Move Jointgrid \"\"\"\n",
    "        h= self.sg.ax_joint.get_position().height\n",
    "        h2= self.sg.ax_marg_x.get_position().height\n",
    "        r = int(np.round(h/h2))\n",
    "        self._resize()\n",
    "        self.subgrid = gridspec.GridSpecFromSubplotSpec(r+1,r+1, subplot_spec=self.subplot)\n",
    "        self._moveaxes(self.sg.ax_joint, self.subgrid[1:, :-1])\n",
    "        self._moveaxes(self.sg.ax_marg_x, self.subgrid[0, :-1])\n",
    "        self._moveaxes(self.sg.ax_marg_y, self.subgrid[1:, -1])\n",
    "    def _moveaxes(self, ax, gs):\n",
    "        #https://stackoverflow.com/a/46906599/4124317\n",
    "        ax.remove()\n",
    "        ax.figure=self.fig\n",
    "        self.fig.axes.append(ax)\n",
    "        self.fig.add_axes(ax)\n",
    "        ax._subplotspec = gs\n",
    "        ax.set_position(gs.get_position(self.fig))\n",
    "        ax.set_subplotspec(gs)\n",
    "    def _finalize(self):\n",
    "        plt.close(self.sg.fig)\n",
    "        self.fig.canvas.mpl_connect(\"resize_event\", self._resize)\n",
    "        self.fig.canvas.draw()\n",
    "    def _resize(self, evt=None):\n",
    "        self.sg.fig.set_size_inches(self.fig.get_size_inches())\n",
    "\n",
    "        \n",
    "def get_contour_plot(h_y,h_yhat,n_y,n_yhat,model,epoch,learning_rate,hidden_dim,output,train_type):\n",
    "     \n",
    "    fig = plt.figure(figsize=(20,15))\n",
    "    gs = gridspec.GridSpec(1, 2)\n",
    "    sns.set(font_scale=1.5)\n",
    "\n",
    "    #1H\n",
    "    g = sns.jointplot(x=h_y, y=h_yhat, color=\"magenta\",kind='reg',scatter=False, xlim = (-4,4), ylim = (-4,4))\n",
    "    r_h, p_h = stats.pearsonr(h_y, h_yhat)\n",
    "    g.ax_joint.annotate(f'pearsonr = {r_h:.2f}, p = {p_h:.2f}',\n",
    "                    xy=(0.1, 0.9), xycoords='axes fraction',\n",
    "                    ha='left', va='center',fontsize=35)\n",
    "    g.plot_joint(sns.kdeplot, color=\"magenta\", fill=True,bw_adjust=.5)\n",
    "    g.set_axis_labels(xlabel='Groundtruth of 1H',ylabel='Prediction of 1H')\n",
    "      \n",
    "\n",
    "    #15N\n",
    "    g1= sns.jointplot(x=n_y, y=n_yhat, color=\"blue\", kind='reg',scatter=False, xlim = (-4,4), ylim = (-4,4))\n",
    "    r_n, p_n = stats.pearsonr(n_y, n_yhat)\n",
    "    g1.ax_joint.annotate(f'pearsonr = {r_n:.2f}, p = {p_n:.2f}',\n",
    "                    xy=(0.1, 0.9), xycoords='axes fraction',\n",
    "                    ha='left', va='center',fontsize=35)\n",
    "    g1.plot_joint(sns.kdeplot, color=\"blue\", fill=True,bw_adjust=.5)\n",
    "    g1.set_axis_labels(xlabel='Groundtruth of 15N',ylabel='Prediction of 15N')\n",
    "    \n",
    "    mg0 = SeabornFig2Grid(g, fig, gs[0])\n",
    "    mg1 = SeabornFig2Grid(g1, fig, gs[1])\n",
    "\n",
    "    gs.tight_layout(fig)\n",
    "    gs.update(top=0.95)\n",
    "    #plt.tight_layout()\n",
    "    plt.savefig(output+\"pearson_\"+model+'_'+str(epoch)+'_'+str(learning_rate)+'_'+str(hidden_dim)+'_'+train_type+'.png',bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    return (r_h, p_h,r_n, p_n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c17b1d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_coverage(precisions, coverages, step, model,num_epochs,learning_rate,hidden_dim,output,train_type,title=\"Coverage and Accuracy\",include_acc=False):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    if include_acc:\n",
    "        colors = [plt.get_cmap('viridis')(0.2), plt.get_cmap('viridis')(0.5), plt.get_cmap('viridis')(0.8)]\n",
    "    else:\n",
    "        colors = [plt.get_cmap('viridis')(0.2), plt.get_cmap('viridis')(0.8)]\n",
    "    plt.plot(step, coverages, label='Coverage', color=colors[0], linewidth=2.5)\n",
    "    plt.plot(step, precisions, label='Accuracy', color=colors[1], linewidth=2.5)\n",
    "    if include_acc:\n",
    "        plt.plot([x for x in [x * step for x in range(int(1 / step) + 1)]], accuracies, label='Accuracy', color=colors[2], linewidth=2.5)\n",
    "    plt.xlabel('Distance', fontsize=14)\n",
    "    plt.ylabel('%', fontsize=14)\n",
    "    plt.xticks(fontsize = 13)\n",
    "    plt.yticks(fontsize = 13)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.xlim(max(step)+0.05, min(step)-0.05)\n",
    "    plt.title(title, fontsize=18, pad=15)\n",
    "    \n",
    "    plt.savefig(output+\"coverage_accuracy_\"+model+'_'+str(num_epochs)+'_'+str(learning_rate)+'_'+str(hidden_dim)+'_'+train_type+'.png',bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4950548",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_boxplot_accuracy(acc, acc_rdnm,model,num_epochs,learning_rate,hidden_dim,output,train_type):\n",
    "\n",
    "    data = [acc, acc_rdnm]\n",
    "\n",
    "    fig = plt.figure(figsize =(5, 7))\n",
    "    # Creating axes instance\n",
    "    ax = fig.add_axes([0, 0, 1, 1])\n",
    "\n",
    "    # Creating plot\n",
    "    bp = ax.boxplot(data,patch_artist=True)\n",
    "\n",
    "    colors = ['darkmagenta','deepskyblue']\n",
    "\n",
    "    for patch, color in zip(bp['boxes'], colors):\n",
    "        patch.set_facecolor(color)\n",
    "\n",
    "    plt.title(\"Accuracy per Protein\")\n",
    "    ax.set_xticklabels(['Proteins', 'Random'])\n",
    "\n",
    "    # show plot\n",
    "    plt.savefig(output+\"accuracy_\"+model+'_'+str(num_epochs)+'_'+str(learning_rate)+'_'+str(hidden_dim)+'_'+train_type+'.png',bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cba26082",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simple(x1, x2, label1, label2, x_label, y_label, title, x_range, output, out_name):\n",
    "\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    l = range(0, len(x1))\n",
    "    plt.plot(l, x1, label=label1)\n",
    "    plt.plot(l, x2, label=label2)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    x_labels = np.arange(min(l), max(l)+1, x_range)\n",
    "    plt.xticks(x_labels)\n",
    "    plt.ylim(ymin=0)\n",
    "    \n",
    "    \n",
    "    plt.legend()\n",
    "\n",
    "    plt.savefig(output+out_name)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    \n",
    "def plot_double(x1, x2, y1, y2, xlabel1, xlabel2, ylabel1, ylabel2, x_label, y_label, title, \n",
    "                x_range, y_range, output, out_name):\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(14, 9))\n",
    "    \n",
    "    l = range(0, len(x1))\n",
    "    m1 = max(y1)\n",
    "    m2 = max(y2)\n",
    "    m = [m1,m2]\n",
    "    \n",
    "    plt.plot(l, y1, label=ylabel1)\n",
    "    plt.plot(l, y2, label=ylabel2)\n",
    "    plt.plot(l, x1, label=xlabel1)\n",
    "    plt.plot(l, x2, label=xlabel2)\n",
    "    \n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.title(title)\n",
    "    \n",
    "    x_labels = np.arange(min(l), max(l)+1, x_range)\n",
    "    y_labels = np.arange(0, max(m)+0.1, y_range)\n",
    "    \n",
    "    plt.xticks(x_labels)\n",
    "    plt.yticks(y_labels)\n",
    "    plt.ylim(ymin=0)\n",
    "    \n",
    "    \n",
    "    plt.legend()\n",
    "    \n",
    "    plt.savefig(output+out_name)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fc774f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class MyCollator(object):\n",
    "    \n",
    "    def __call__(self, batch, ignore_idx=-100):\n",
    "        # batch is a list of the samples returned by your __get_item__ method in your CustomDataset\n",
    "        ids, X, Y = zip(*batch)\n",
    "        X = pad_sequence(X, batch_first=True, padding_value=ignore_idx)\n",
    "        Y = pad_sequence(Y, batch_first=True, padding_value=ignore_idx)\n",
    "        return (list(ids), X, Y)\n",
    "\n",
    "    \n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    \n",
    "    def __init__(self, samples, first):\n",
    "        \n",
    "        #self.ids = []\n",
    "        #self.inputs = []\n",
    "        #self.targets = []\n",
    "        \n",
    "        self.item = []\n",
    "        self.seq = []\n",
    "        self.first = first\n",
    "        \n",
    "        for seq, item in samples.items():\n",
    "            self.seq.append(seq)\n",
    "            self.item.append(item)\n",
    "            \n",
    "        self.data_len = len(self.item)    \n",
    "        \n",
    "        \n",
    "        #self.ids, self.inputs, self.targets = zip(*[(ids, inputs, targets)  \n",
    "        #                    for ids, (inputs, targets) in samples.items()])\n",
    "        #self.data_len = len(self.inputs) # number of samples in the set\n",
    "        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        curr_item = self.item[index]\n",
    "        \n",
    "        if self.first:\n",
    "            i = 0\n",
    "        else:\n",
    "            length = len(curr_item)\n",
    "            i = randint(0, length-1)\n",
    "            \n",
    "        item = curr_item[i]\n",
    "        ids = item[0]\n",
    "        x = item[1]\n",
    "        y = item[2]    \n",
    "            \n",
    "        \n",
    "        #ids = self.ids[index]\n",
    "        #x = self.inputs[index]#.float()\n",
    "        #y = self.targets[index]#.long()\n",
    "        return (ids, torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32))\n",
    "\n",
    "    \n",
    "def get_dataloader(customdata, batch_size, first):\n",
    "    # Create dataloaders with collate function\n",
    "    my_collator = MyCollator()\n",
    "    dataset = CustomDataset(customdata, first)\n",
    "    return torch.utils.data.DataLoader(dataset=dataset,\n",
    "                                        batch_size=batch_size, \n",
    "                                        shuffle=True,\n",
    "                                        drop_last=False,\n",
    "                                        collate_fn=my_collator)\n",
    "  \n",
    "\n",
    "class EarlyStopper():\n",
    "    def __init__(self, log_dir, model_choice, input_dim, hidden_dim, output_dim):\n",
    "        self.log_dir      = log_dir\n",
    "        self.checkpoint_p = log_dir / 'checkpoint.pt'\n",
    "        self.epsilon      = 1e-3 # the minimal difference in improvement a model needs to reach\n",
    "        self.min_loss     = np.Inf # counter for lowest/best overall-loss\n",
    "        self.n_worse      = 0 # counter of consecutive non-improving losses\n",
    "        self.patience     = 5 # number of max. epochs accepted for non-improving loss\n",
    "        self.model_choice = model_choice\n",
    "        self.input_dim    = input_dim \n",
    "        self.hidden_dim   = hidden_dim\n",
    "        self.output_dim   = output_dim\n",
    "       \n",
    "    \n",
    "    def load_checkpoint(self):\n",
    "        state = torch.load( self.checkpoint_p)\n",
    "        model = get_model(model_choice=self.model_choice, input_dim = self.input_dim, hidden_dim = self.hidden_dim, output_dim=self.output_dim)\n",
    "        model.load_state_dict(state['state_dict'])\n",
    "        print('Loaded model from epoch: {:.1f}'.format(state['epoch']))\n",
    "        return model, state['epoch']\n",
    "    \n",
    "    def save_checkpoint(self, model, epoch, optimizer):\n",
    "        state = { \n",
    "                    'epoch'      : epoch,\n",
    "                    'state_dict' : model.state_dict(),\n",
    "                    'optimizer'  : optimizer.state_dict(),\n",
    "                }\n",
    "        torch.save( state, self.checkpoint_p )\n",
    "        return None\n",
    "    \n",
    "\n",
    "         \n",
    "        \n",
    "    def check_performance(self, model, test_loader, crit, optimizer, epoch, num_epochs):\n",
    "        current_loss,v_epoch_loss_h,v_epoch_loss_n, acc, acc_rndm, coverage, precision,bins, hn = testing(model, test_loader, crit, epoch, num_epochs, set_name=\"VALID\")\n",
    "    \n",
    "        # if the model improved compared to previously best checkpoint\n",
    "        if current_loss < (self.min_loss - self.epsilon):\n",
    "            print('New best model found with loss= {:.3f}'.format(current_loss))  \n",
    "            self.save_checkpoint( model, epoch, optimizer)\n",
    "            self.min_loss = current_loss # save new loss as best checkpoint\n",
    "            self.n_worse  = 0\n",
    "        else: # if the model did not improve further increase counter\n",
    "            self.n_worse += 1\n",
    "            if self.n_worse > self.patience: # if the model did not improve for 'patience' epochs\n",
    "                print('Stopping due to early stopping after epoch {}!'.format(epoch))\n",
    "                return True, current_loss, v_epoch_loss_h,v_epoch_loss_n, acc, acc_rndm, coverage, precision,bins, hn\n",
    "        return False, current_loss,v_epoch_loss_h,v_epoch_loss_n, acc, acc_rndm, coverage, precision, bins, hn\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Mask residues with -100, replace with 0     \n",
    "def mask(Y, Yhat):\n",
    "       \n",
    "    true_false = Y!=-100\n",
    "    masked_y = torch.where(true_false,Y, 0)\n",
    "    masked_yhat = torch.where(true_false, Yhat, 0)\n",
    "    \n",
    "    return masked_y, masked_yhat     \n",
    "\n",
    "\n",
    "#calcluate Loss for H and N individually\n",
    "def get_loss(Y, Yhat, crit):\n",
    "    \n",
    "    y_masked_h, yhat_masked_h = mask(Y[:,:,0],  Yhat[:,:,0])\n",
    "    y_masked_n, yhat_masked_n = mask(Y[:,:,1],  Yhat[:,:,1])\n",
    "    \n",
    "    \n",
    "    loss_h = crit(y_masked_h, yhat_masked_h)\n",
    "    loss_n = crit(y_masked_n, yhat_masked_n)\n",
    "    \n",
    "    return loss_h, loss_n   \n",
    "\n",
    "\n",
    "        \n",
    "def training(model, trainloader, crit, optimizer, epoch, num_epochs):\n",
    "    \n",
    "    model.train() # ensure model is in training mode (dropout, batch norm, ...)\n",
    "    accuracies = []\n",
    "    batch = 0\n",
    "    batch_loss = 0\n",
    "    batch_loss_h = 0\n",
    "    batch_loss_n = 0\n",
    "    \n",
    "    #start = time.time()\n",
    " \n",
    "    for i, (_, X, Y) in enumerate(trainloader): # iterate over all mini-batches in train\n",
    "      \n",
    "        optimizer.zero_grad() # zeroes the gradient buffers of all parameters\n",
    "        \n",
    "        X    = X.to(device)\n",
    "        Y    = Y.to(device)\n",
    "        \n",
    "        #train\n",
    "        Yhat = model(X)\n",
    "        \n",
    "        #loss\n",
    "        #Loss normalisation\n",
    "        #-loss konstanter Faktor multiplikation\n",
    "        #-raw input data normalization (zscore)\n",
    "        #total_loss = (loss_h + loss_n)/2 #mean(loss) oder selber effekt: nur summe(loss) dafür learning rate runter\n",
    "    \n",
    "        loss_h, loss_n = get_loss(Y, Yhat, crit)\n",
    "        total_loss = (loss_h + loss_n)/2\n",
    "        \n",
    "        #backpropagation\n",
    "        total_loss.backward() \n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        #sum up loss per batch\n",
    "        batch += 1\n",
    "        batch_loss += total_loss.detach().cpu().numpy()\n",
    "        batch_loss_h += loss_h.detach().cpu().numpy()\n",
    "        batch_loss_n += loss_n.detach().cpu().numpy()\n",
    "        \n",
    "        \n",
    "    #collect loss for plot: mean over batches per epoch\n",
    "    epoch_loss = batch_loss/batch\n",
    "    epoch_loss_h = batch_loss_h/batch\n",
    "    epoch_loss_n = batch_loss_n/batch\n",
    "        \n",
    "    #end = time.time()\n",
    "    if epoch % 1 == 0 or epoch == num_epochs:\n",
    "        out = ('Epoch [{}/{}], TRAIN loss: {:.2f}').format( \n",
    "                    epoch, num_epochs, \n",
    "                    epoch_loss,\n",
    "                    #end-start,\n",
    "                    )\n",
    "        print(out)\n",
    "        \n",
    "    return epoch_loss,epoch_loss_h,epoch_loss_n \n",
    "\n",
    "\n",
    "\n",
    "def testing( model, testloader, crit, epoch, num_epochs, log_dir=None, set_name=None):\n",
    "    model.eval() # [ensure model is in training mode (dropout, batch norm, ...)\n",
    "    accuracies = []\n",
    "    accuracies_rndm = []\n",
    "    \n",
    "    \n",
    "    h_y = []\n",
    "    h_yhat = []\n",
    "    n_y = []\n",
    "    n_yhat = []\n",
    "    \n",
    "    auc_list = []\n",
    "    coverage_all = []\n",
    "    precision_all = []\n",
    "    \n",
    "    distances_all = [] #list of distances to 1-NN\n",
    "    truepredictions = [] #correctly labeled 1-NNs\n",
    "    prot_len_list = []\n",
    "    \n",
    "\n",
    "    #start = time.time()\n",
    "    results = {}\n",
    "    \n",
    "    batch = 0\n",
    "    batch_loss = 0\n",
    "    batch_loss_h = 0\n",
    "    batch_loss_n = 0\n",
    "    \n",
    "    for i, (pdb_ids, X, Y) in enumerate(testloader):\n",
    "        \n",
    "        # IN: [B, L, F] OUT: [B, N, L]\n",
    "        X    = X.to(device)\n",
    "        Y    = Y.to(device)\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            Yhat = model(X)\n",
    "   \n",
    "        loss_h, loss_n = get_loss(Y, Yhat, crit)\n",
    "        total_loss = (loss_h + loss_n)/2\n",
    "        \n",
    "        \n",
    "        # sum up loss per batch\n",
    "        batch += 1\n",
    "        batch_loss += total_loss.detach().cpu().numpy()\n",
    "        batch_loss_h += loss_h.detach().cpu().numpy()\n",
    "        batch_loss_n += loss_n.detach().cpu().numpy()\n",
    "        \n",
    "        \n",
    "        # IN: [B, N, L] OUT: [B, L]\n",
    "        # iterate over every sample in batch\n",
    "        for sample_idx in range(0,Yhat.shape[0]):\n",
    "            \n",
    "            yhat = Yhat[sample_idx] # get single sample/protein from mini-batch\n",
    "            y    = Y[sample_idx]\n",
    "            \n",
    "            #Mask vectors\n",
    "            y_masked_h, yhat_masked_h = mask(y[:,0],  yhat[:,0])\n",
    "            y_masked_n, yhat_masked_n = mask(y[:,1],  yhat[:,1])\n",
    "            \n",
    "            y_masked_h = y_masked_h[y_masked_h.nonzero().squeeze()]\n",
    "            y_masked_n = y_masked_n[y_masked_n.nonzero().squeeze()]\n",
    "            yhat_masked_h = yhat_masked_h[yhat_masked_h.nonzero().squeeze()]\n",
    "            yhat_masked_n = yhat_masked_n[yhat_masked_n.nonzero().squeeze()]\n",
    "            \n",
    "            y_masked = torch.stack((y_masked_h, y_masked_n),-1)\n",
    "            yhat_masked = torch.stack((yhat_masked_h, yhat_masked_n),-1)\n",
    "            \n",
    "            \n",
    "            #Collect all values for pearson per epoch\n",
    "            h_y.extend(y_masked_h.tolist())\n",
    "            h_yhat.extend(yhat_masked_h.tolist())\n",
    "            n_y.extend(y_masked_n.tolist())\n",
    "            n_yhat.extend(yhat_masked_n.tolist())\n",
    "            \n",
    "            #Classification Accuracy\n",
    "            #nearest neighbor: right/wrong assignment\n",
    "            nbrs = NearestNeighbors(n_neighbors=1, algorithm='auto').fit(y_masked.cpu()) \n",
    "            distances, indices = nbrs.kneighbors(yhat_masked.cpu())\n",
    "            \n",
    "            \n",
    "            #collect true prediction, if kNN matches position\n",
    "            indices = indices.flatten()\n",
    "            prot_len = len(indices)\n",
    "            index_comparison = np.arange(prot_len)\n",
    "            \n",
    "        \n",
    "            \n",
    "            #acc = correct predictions/number of predictions\n",
    "            comp = (indices==index_comparison)\n",
    "            true = (comp == True).sum()\n",
    "            acc = true/prot_len\n",
    "            accuracies.append(acc)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #random index list to compare accuracy against\n",
    "            res = random.sample(range(0, prot_len),prot_len)\n",
    "            comp_rndm = (res==index_comparison)\n",
    "            true_rndm = (comp_rndm == True).sum()\n",
    "            acc_rndm = true_rndm/prot_len\n",
    "            accuracies_rndm.append(acc_rndm)\n",
    "            \n",
    "           \n",
    "        \n",
    "            #Precision, Coverage\n",
    "            #Coverage: wie viele meiner Residues kann ich bei gegebenem Distance-cut-off X noch vorhersagen \n",
    "            #y% aller Residues haben Distance<X, zB 30% aller Residues haben eine Distanz<0.4 zum nearest neighbor\n",
    "            #Precision: bei gegbenem Distance-cut-off X, sind wie viele deiner vorhergesagten Residues korrekt \n",
    "            #z% der Residues mit Distance<X sind korrekt., zB 10% der Residues die bei Distanz<0.4 vorhergesagt werden sind korrekt.\n",
    "            \n",
    "            distances = distances.flatten()\n",
    "            \n",
    "            distances_all.extend(distances)\n",
    "            \n",
    "            truepredictions.extend(comp)\n",
    "            p = np.full(len(distances), prot_len)\n",
    "            prot_len_list.extend(p)\n",
    "            \n",
    "            \n",
    "            if epoch==num_epochs: # store predictions of final checkpoint for writing to log\n",
    "                pdb_id = pdb_ids[sample_idx]\n",
    "                results[pdb_id] = (','.join([str(i.detach().cpu().numpy()) for i in y]),\n",
    "                                   ','.join([str(j.detach().cpu().numpy()) for j in yhat])\n",
    "                                   ,accuracies[-1])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    #Coverage/Precision (Accuracy)\n",
    "    \n",
    "    #sort distances\n",
    "    sorted_lists = sorted(zip(distances_all, truepredictions), key=lambda x: x[0], reverse=True)\n",
    "    dist, trues = zip(*sorted_lists)\n",
    "    \n",
    "    max_dist = dist[0]\n",
    "    #min_dist = dist[-1]\n",
    "    \n",
    "    bins = np.arange(0, max_dist + 0.01, 0.01)[::-1]\n",
    "    set_len = len(dist)\n",
    "    \n",
    "   \n",
    "    for b in bins:\n",
    "        \n",
    "        start_index = next((index for index, value in enumerate(dist) if value < b), set_len)\n",
    "        n = len(dist[start_index:])\n",
    "\n",
    "        # calculate coverage/precision\n",
    "        if n == 0:\n",
    "            prec = 0\n",
    "            cov = 0\n",
    "        else:\n",
    "            # Use list comprehension to count True values within the range of the first bin\n",
    "            trues_in_bin = sum(trues[start_index:])\n",
    "            \n",
    "            prec = trues_in_bin/n\n",
    "            cov = n/set_len\n",
    "        \n",
    "\n",
    "        precision_all.append(prec)\n",
    "        coverage_all.append(cov)\n",
    "        \n",
    "    \n",
    "    \n",
    "    #Accuracy Mean\n",
    "    acc_mean = sum(accuracies)/len(accuracies)\n",
    "    acc_mean_rndm = sum(accuracies_rndm)/len(accuracies_rndm)\n",
    "\n",
    "    \n",
    "    #collect loss for plot: mean over batches per epoch\n",
    "    epoch_loss = batch_loss/batch\n",
    "    epoch_loss_h = batch_loss_h/batch\n",
    "    epoch_loss_n = batch_loss_n/batch\n",
    "            \n",
    "    #end = time.time()\n",
    "    if epoch % 1 == 0 or epoch == num_epochs:\n",
    "        out = ('Epoch [{}/{}], {} loss: {:.2f}, Accuracy Mean: {:.2f}, Accuracy RNDM: {:.2f}').format( \n",
    "                    epoch,num_epochs, set_name,\n",
    "                    epoch_loss,\n",
    "                    acc_mean, acc_mean_rndm\n",
    "                    #end-start,\n",
    "                    )\n",
    "        print(out)\n",
    "        \n",
    "    if epoch==num_epochs:\n",
    "        write_predictions(results, log_dir, set_name)\n",
    "    \n",
    "    \n",
    "\n",
    "  \n",
    "    return epoch_loss,epoch_loss_h,epoch_loss_n, accuracies, accuracies_rndm, coverage_all, precision_all, bins, (h_y,h_yhat,n_y,n_yhat) \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "def write_predictions(results, log_dir, set_name):\n",
    "    out_p = log_dir / (set_name + '_log.txt')\n",
    "    with open(out_p, 'w+') as out_f:\n",
    "        out_f.write('\\n'.join( \n",
    "            \">{},y,yhat,acc={:.3f}\\n{}\\n{}\".format(pdb_id, acc, y, yhat) \n",
    "                              for pdb_id, (y,yhat,acc) in results.items() ) )\n",
    "    return None\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_model(model_choice,input_dim, hidden_dim, output_dim):\n",
    "     if model_choice == \"CNN\":\n",
    "        return CNN(input_dim, output_dim).to(device)\n",
    "     elif model_choice == \"FNN\":\n",
    "        return TwoLayerFNN(input_dim, hidden_dim, output_dim).to(device)\n",
    "     elif model_choice == \"LinReg\":\n",
    "        return LinearRegression().to(device)\n",
    "     else:\n",
    "        raise NotImplementedError\n",
    "\n",
    "        \n",
    "        \n",
    "#Get Loss of Epoch 0        \n",
    "def get_initialized_loss(model, dataloader, crit):\n",
    "    batch = 0\n",
    "    batch_loss = 0\n",
    "    batch_loss_h = 0\n",
    "    batch_loss_n = 0\n",
    "    \n",
    "    for i, (pdb_ids, X, Y) in enumerate(dataloader):\n",
    "        \n",
    "        X    = X.to(device)\n",
    "        Y    = Y.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            Yhat = model(X)\n",
    "\n",
    "        loss_h, loss_n = get_loss(Y, Yhat, crit)\n",
    "        total_loss = (loss_h + loss_n)/2\n",
    "        \n",
    "        batch += 1\n",
    "        batch_loss += total_loss.detach().cpu().numpy()\n",
    "        batch_loss_h += loss_h.detach().cpu().numpy()\n",
    "        batch_loss_n += loss_n.detach().cpu().numpy()\n",
    "\n",
    "\n",
    "    epoch_loss = batch_loss/batch\n",
    "    epoch_loss_h = batch_loss_h/batch\n",
    "    epoch_loss_n = batch_loss_n/batch\n",
    "    \n",
    "    return epoch_loss,epoch_loss_h,epoch_loss_n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30cdc78f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model_choice, input_dim, hidden_dim, output_dim,batch_size,learning_rate, num_epochs,train,valid,test, output, train_type):\n",
    "\n",
    "    train_loader = get_dataloader(train,batch_size=batch_size,first=True)\n",
    "    val_loader = get_dataloader(valid,batch_size=batch_size,first=True)\n",
    "    test_loader = get_dataloader(test,batch_size=batch_size,first=True)\n",
    "    \n",
    "\n",
    "    #val_aa_mean = get_dataloader(valid_aa_mean,batch_size=batch_size,first=True)\n",
    "\n",
    "\n",
    "    root = Path.cwd() \n",
    "    # create log directory if it does not exist yet\n",
    "    log_root = root / \"log\"\n",
    "    if not log_root.is_dir():\n",
    "        print(\"Creating new log-directory: {}\".format(log_root))\n",
    "        log_root.mkdir()\n",
    "\n",
    "    log_dir = log_root\n",
    "\n",
    "\n",
    "\n",
    "    model = get_model(model_choice, input_dim, hidden_dim, output_dim)\n",
    "\n",
    "    early_stopper = EarlyStopper(log_dir, model_choice, input_dim, hidden_dim, output_dim)\n",
    "\n",
    "    n_free_paras = count_parameters(model)\n",
    "    print('Number of free parameters: {}'.format( n_free_paras))\n",
    "\n",
    "    crit = nn.MSELoss() \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, amsgrad=True)\n",
    "\n",
    "\n",
    "    #Loss\n",
    "    train_loss = []\n",
    "    train_loss_h = []\n",
    "    train_loss_n = []\n",
    "\n",
    "    valid_loss = []\n",
    "    valid_loss_h = []\n",
    "    valid_loss_n = []\n",
    "\n",
    "    \n",
    "\n",
    "    #Initial Loss of Epoch 0\n",
    "    #for Loss Plot: take mean per epoch of all batches\n",
    "    #for Loss propagation: take every single loss per batch\n",
    "    t_epoch_loss,t_epoch_loss_h,t_epoch_loss_n = get_initialized_loss(model, train_loader,crit)\n",
    "    v_epoch_loss,v_epoch_loss_h,v_epoch_loss_n = get_initialized_loss(model, val_loader, crit)\n",
    "\n",
    "    train_loss.append(math.sqrt(t_epoch_loss))\n",
    "    train_loss_h.append(math.sqrt(t_epoch_loss_h))\n",
    "    train_loss_n.append(math.sqrt(t_epoch_loss_n))\n",
    "\n",
    "    valid_loss.append(math.sqrt(v_epoch_loss))\n",
    "    valid_loss_h.append(math.sqrt(v_epoch_loss_h))\n",
    "    valid_loss_n.append(math.sqrt(v_epoch_loss_n))\n",
    "\n",
    "\n",
    "    #TODO: Log datei \n",
    "    \n",
    "    print(train_type)\n",
    "\n",
    "    #start = time.time()\n",
    "    for epoch in range(num_epochs): # for each epoch: train & valid\n",
    "        stop, v_epoch_loss,v_epoch_loss_h,v_epoch_loss_n, acc, acc_rndm, coverage, precision,bins, hn = early_stopper.check_performance(model, val_loader, crit, optimizer, epoch, num_epochs)\n",
    "        if stop: # if early stopping criterion was reached\n",
    "            break\n",
    "        t_epoch_loss,t_epoch_loss_h,t_epoch_loss_n  = training(model, train_loader, crit, optimizer, epoch, \n",
    "                                                               num_epochs) \n",
    "        \"\"\"v_epoch_loss,v_epoch_loss_h,v_epoch_loss_n, acc, acc_rndm, coverage, precision, hn = testing(model, \n",
    "                                                                                val_loader, crit, \n",
    "                                                                                epoch, num_epochs, \n",
    "                                                                                log_dir=log_dir, set_name=\"VAL\")\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "        # Collect loss per epoch\n",
    "        train_loss.append(math.sqrt(t_epoch_loss))\n",
    "        train_loss_h.append(math.sqrt(t_epoch_loss_h))\n",
    "        train_loss_n.append(math.sqrt(t_epoch_loss_n))\n",
    "\n",
    "        valid_loss.append(math.sqrt(v_epoch_loss))\n",
    "        valid_loss_h.append(math.sqrt(v_epoch_loss_h))\n",
    "        valid_loss_n.append(math.sqrt(v_epoch_loss_n))\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "        #Random Baseline: H/N mean per AA\n",
    "        \"\"\"v_epoch_loss,v_epoch_loss_h,v_epoch_loss_n, acc, acc_rndm, coverage_all, precision_all, hn = testing(model, \n",
    "                                                                                val_aa_mean, crit, \n",
    "                                                                                epoch, num_epochs, \n",
    "                                                                                log_dir=log_dir, set_name=\"VAL_AA_Mean\")\n",
    "        acc_valid_aa_mean.extend(acc)\"\"\"\n",
    "\n",
    "\n",
    "    # load the model weights of the best checkpoint\n",
    "    model = early_stopper.load_checkpoint()[0]\n",
    "    #end = time.time()\n",
    "    #print('Total training time: {}[m]'.format((end-start)/60))\n",
    "    print('Running final evaluation on the best checkpoint.')\n",
    "    _,_,_, acc, acc_rndm, coverage, precision,bins, hn = testing(model,test_loader, crit, epoch, num_epochs, \n",
    "                                                                                log_dir=log_dir, set_name=\"Test\")\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    #Plots\n",
    "    #Precision/Coverage\n",
    "    #get_plot_precision_coverage(coverage,precision,model_choice,num_epochs,learning_rate,hidden_dim,output,train_type)\n",
    "    plot_precision_coverage(precision, coverage, bins, model_choice, num_epochs,learning_rate,hidden_dim,output,train_type,title=\"Coverage and Accuracy\",include_acc=False)\n",
    "    \n",
    "    #Accuracy\n",
    "    #get_boxplot_accuracy(acc_valid, acc_rdnm_valid, acc_valid_aa_mean, model_choice,num_epochs,learning_rate,hidden_dim,output,train_type)\n",
    "    get_boxplot_accuracy(acc, acc_rndm, model_choice,num_epochs,learning_rate,hidden_dim,output,train_type)\n",
    "    \n",
    "    \n",
    "    \n",
    "    #Pearson correlation: predicted vs groundtruth\n",
    "    pearson = get_contour_plot(hn[0],hn[1],hn[2],hn[3], model_choice,epoch,learning_rate,hidden_dim,output,train_type)\n",
    "    \n",
    "    #Loss\n",
    "    t = 'Training and Validation Loss ('+train_type+', '+model_choice+')'\n",
    "    out_loss = 'loss_'+train_type+'_'+str(num_epochs)+'_'+str(learning_rate)+'_'+str(hidden_dim)+'.png'\n",
    "    out_losshn = 'lossHN_'+train_type+'_'+str(num_epochs)+'_'+str(learning_rate)+'_'+str(hidden_dim)+'.png'\n",
    "    \n",
    "    \n",
    "    plot_simple(train_loss, valid_loss, 'Training Loss', 'Validation Loss', 'Epochs', 'Loss', \n",
    "            t, 1, output, out_loss)\n",
    "    \n",
    "    #Loss seperate H/N\n",
    "    plot_double(train_loss_h,valid_loss_h,train_loss_n, valid_loss_n, \n",
    "            'Training Loss H', 'Training Loss N', \n",
    "            'Validation Loss H','Validation Loss N',\n",
    "            'Epochs', 'Loss',t, \n",
    "            1, 0.05, output,out_losshn)\n",
    "    \n",
    "    return acc, acc_rndm, pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53964fd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Hyper parameters\\nnum_epochs    = 1\\nlearning_rate = 1e-3\\nbatch_size    = 128 \\n\\n# Initialize the model\\ninput_dim = 1024  # Number of input features\\noutput_dim = 2  # Number of output values\\nhidden_dim = 513  # Number of neurons in the hidden layer: (input_dim + output_dim) / 2,\\n\\nmodel_choice = \"FNN\" \\n\\noutput = \"./\"\\n\\npredict(model_choice, input_dim, hidden_dim, output_dim,batch_size,learning_rate,\\n        num_epochs,train,test,valid, output)'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Hyper parameters\n",
    "num_epochs    = 1\n",
    "learning_rate = 1e-3\n",
    "batch_size    = 128 \n",
    "\n",
    "# Initialize the model\n",
    "input_dim = 1024  # Number of input features\n",
    "output_dim = 2  # Number of output values\n",
    "hidden_dim = 513  # Number of neurons in the hidden layer: (input_dim + output_dim) / 2,\n",
    "\n",
    "model_choice = \"FNN\" \n",
    "\n",
    "output = \"./\"\n",
    "\n",
    "predict(model_choice, input_dim, hidden_dim, output_dim,batch_size,learning_rate,\n",
    "        num_epochs,train,test,valid, output)\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NMR_master",
   "language": "python",
   "name": "nmr_master"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
